# My Reflection on Using Github for Data Cleaning Portfolio Project

# What my copilot created/genertated
For this portfolio project I used Github to help me createmultiple Python functions in my data cleaning project. To further explain, Copilot suggested versions of "Load_data", "clean_column_names", and also "handle_missing_values". I helped these suggestions by writing comments saying what I wanted each function to do. For example, like the function "# Fuction to load raw CSV data". Copilot helped me by giving mr code suggestions I could use for my project

# What I changed
So while doing this project, I can say that Copilot helped provide a good starting point, I had to actually modify the code to be able to match the specific requirements for my assignemnt. For example, some of the variables I had to rename so that they were more consistent, like "qty" instead of "quantity". I also had to change the logic of it for handling missing values and also missing numbers so that they would match more accurately to the column names in the raw CSV file. To help you understand, all these cahnges were very important because the original suggestions it gave me assumed different column names, which would have caused errors in my program if I left it unchanged.

# What I learned
This project was quite tricky for me, but I learned a lot. I learned how to use pandas to clean my data. I also practiced standardizing column names, dropping missing values, and another thing I learned was filtering invalid rows. This project also taught me how to organize a data cleaning pipeline, making the code easier to read. Using Github Copilot was very helpful for me to help quickly generate code. It sometimes made mistakes but by editing them helped me learn and understand exactly how the code actually works.

#  Future Imporovements
This project was a learning process for me, but in the future I believe that I could make this script more useful and reusable by creating different functions that will accept any dataset being used with similar structure. I could do this by adding automated checks for invalid data, and/or writing tests that would help ensure that the pipeline works correctly with any dataset. In the future, I could also explore more different and advanced data cleaning techniques, like inconsistent text entries to make the code more flexible.